{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "def create_database():\n",
    "    # Connect to the default database to create the new database\n",
    "    conn = psycopg2.connect(\n",
    "        dbname='postgres',  # Connect to the default database\n",
    "        user='admin',\n",
    "        password='admin',\n",
    "        host='localhost',\n",
    "        port='5432'\n",
    "    )\n",
    "    conn.autocommit = True  # Enable autocommit to create database\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    try:\n",
    "        # Create the goodread database if it doesn't exist\n",
    "        cur.execute(\"SELECT 1 FROM pg_catalog.pg_database WHERE datname = 'goodread'\")\n",
    "        exists = cur.fetchone()\n",
    "        if not exists:\n",
    "            cur.execute(\"CREATE DATABASE goodread;\")\n",
    "            print(\"Database 'goodread' has been created successfully.\")\n",
    "        else:\n",
    "            print(\"Database 'goodread' already exists.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while creating database: {e}\")\n",
    "    finally:\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "\n",
    "def create_tables():\n",
    "    conn = psycopg2.connect(\n",
    "        dbname='goodread',\n",
    "        user='admin',\n",
    "        password='admin',\n",
    "        host='localhost',\n",
    "        port='5432'\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        \n",
    "        # Create Authors table\n",
    "        create_authors_table = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS authors (\n",
    "            author_id SERIAL PRIMARY KEY,\n",
    "            author_name VARCHAR(255) UNIQUE\n",
    "        );\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create Ratings table\n",
    "        create_ratings_table = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS ratings (\n",
    "            rating_id SERIAL PRIMARY KEY,\n",
    "            rating FLOAT,\n",
    "            fivestars INT,\n",
    "            fourstars INT,\n",
    "            threestars INT,\n",
    "            twostars INT,\n",
    "            onestar INT\n",
    "        );\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create Books table\n",
    "        create_books_table = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS books (\n",
    "            book_id SERIAL PRIMARY KEY,\n",
    "            rating_id INT,\n",
    "            author_id INT,\n",
    "            bookname VARCHAR(255),\n",
    "            publish DATE,\n",
    "            prices FLOAT,\n",
    "            rating FLOAT,\n",
    "            rating_count INT,\n",
    "            reviews INT,\n",
    "            pages_n INT,\n",
    "            cover VARCHAR(50),\n",
    "            bookUrl VARCHAR(255),\n",
    "            FOREIGN KEY (author_id) REFERENCES authors(author_id),\n",
    "            FOREIGN KEY (rating_id) REFERENCES ratings(rating_id)\n",
    "        );\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create Details table\n",
    "        create_details_table = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS details (\n",
    "            book_id INT PRIMARY KEY,\n",
    "            author_id INT,\n",
    "            describe TEXT,\n",
    "            book_title VARCHAR(255),\n",
    "            FOREIGN KEY (book_id) REFERENCES books(book_id),\n",
    "            FOREIGN KEY (author_id) REFERENCES authors(author_id)\n",
    "        );\n",
    "        \"\"\"\n",
    "        \n",
    "        # Execute table creation commands\n",
    "        cur.execute(create_authors_table)\n",
    "        cur.execute(create_ratings_table)\n",
    "        cur.execute(create_books_table)\n",
    "        cur.execute(create_details_table)\n",
    "        conn.commit()\n",
    "\n",
    "        print(\"Tables have been created successfully.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while creating tables: {e}\")\n",
    "    finally:\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "\n",
    "# Create the database and tables\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database 'goodread' has been created successfully.\n",
      "Tables have been created successfully.\n"
     ]
    }
   ],
   "source": [
    "create_database()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables have been created successfully.\n"
     ]
    }
   ],
   "source": [
    "create_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "def create_connection():\n",
    "    return psycopg2.connect(\n",
    "        dbname='goodread',\n",
    "        user='admin',\n",
    "        password='admin',\n",
    "        host='localhost',\n",
    "        port='5432'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_authors():\n",
    "    authors_data = [\n",
    "        ('J.K. Rowling',),\n",
    "        ('George Orwell',),\n",
    "        ('J.R.R. Tolkien',)\n",
    "    ]\n",
    "    \n",
    "    conn = create_connection()\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    try:\n",
    "        for author in authors_data:\n",
    "            cur.execute(\"INSERT INTO authors (author_name) VALUES (%s) ON CONFLICT (author_name) DO NOTHING;\", author)\n",
    "        \n",
    "        conn.commit()\n",
    "        print(\"Authors data inserted successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error inserting authors: {e}\")\n",
    "        conn.rollback()\n",
    "    finally:\n",
    "        cur.close()\n",
    "        conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import regexp_replace, col, regexp_extract, when, date_format, to_date\n",
    "import psycopg2\n",
    "\n",
    "# Connect to Kafka\n",
    "def connect_kafka():\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName('SparkKafkaToPostgres') \\\n",
    "        .config('spark.jars.packages', \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.3,\"\n",
    "                                        \"org.postgresql:postgresql:42.5.0\") \\\n",
    "        .getOrCreate()\n",
    "    return spark\n",
    "\n",
    "# Load data from Kafka\n",
    "def load_data(spark):\n",
    "    df = spark.readStream.format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "        .option(\"subscribe\", \"goodread\") \\\n",
    "        .load()\n",
    "    return df\n",
    "\n",
    "# Format data\n",
    "def format_data(df):\n",
    "    df = df.selectExpr(\"CAST(value AS STRING)\") \\\n",
    "        .selectExpr(\"from_json(value, 'author STRING, bookUrl STRING, bookname STRING, describe STRING, prices STRING, publish STRING, rating STRING, ratingcount STRING, reviews STRING, fivestars STRING, fourstars STRING, threestars STRING, twostars STRING, onestar STRING, pages STRING') as jsonData\") \\\n",
    "        .select(\"jsonData.*\")\n",
    "\n",
    "    # Processing string data\n",
    "    df = df.withColumn(\"onestar\", regexp_replace(col(\"onestar\"), r\"[^\\d]\", \"\"))\n",
    "    df = df.withColumn(\"twostars\", regexp_replace(col(\"twostars\"), r\"[^\\d]\", \"\"))\n",
    "    df = df.withColumn(\"threestars\", regexp_replace(col(\"threestars\"), r\"[^\\d]\", \"\"))\n",
    "    df = df.withColumn(\"fourstars\", regexp_replace(col(\"fourstars\"), r\"[^\\d]\", \"\"))\n",
    "    df = df.withColumn(\"fivestars\", regexp_replace(col(\"fivestars\"), r\"[^\\d]\", \"\"))\n",
    "    df = df.withColumn(\"pages_n\", regexp_extract(col(\"pages\"), r\"(\\d+)\", 1))\n",
    "    df = df.withColumn(\"cover\", regexp_extract(col(\"pages\"), r\",\\s*(.*)\", 1))\n",
    "    df = df.withColumn(\"prices\", when(df[\"prices\"].like(\"Kindle%\"), regexp_extract(col(\"prices\"), r\"\\$(\\d+\\.\\d{2})\", 1)).otherwise(0))\n",
    "    df = df.withColumn(\"publish\", regexp_extract(col(\"publish\"), r\"(\\w+ \\d{1,2}, \\d{4})\", 1))\n",
    "    df = df.withColumn(\"publish\", to_date(col(\"publish\"), \"MMMM d, yyyy\"))\n",
    "    df = df.withColumn(\"publish\", date_format(col(\"publish\"), \"dd/MM/yyyy\"))\n",
    "    df = df.withColumn(\"ratingcount\", regexp_replace(col(\"ratingcount\"), \",\", \"\"))\n",
    "    df = df.withColumn(\"reviews\", regexp_replace(col(\"reviews\"), \",\", \"\"))\n",
    "    df = df.drop(\"pages\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# Convert data types\n",
    "def convert(df):\n",
    "    df = df.withColumn(\"pages_n\", df[\"pages_n\"].cast(\"int\")) \\\n",
    "           .withColumn(\"prices\", df[\"prices\"].cast(\"float\")) \\\n",
    "           .withColumn(\"onestar\", df[\"onestar\"].cast(\"int\")) \\\n",
    "           .withColumn(\"twostars\", df[\"twostars\"].cast(\"int\")) \\\n",
    "           .withColumn(\"threestars\", df[\"threestars\"].cast(\"int\")) \\\n",
    "           .withColumn(\"fourstars\", df[\"fourstars\"].cast(\"int\")) \\\n",
    "           .withColumn(\"fivestars\", df[\"fivestars\"].cast(\"int\")) \\\n",
    "           .withColumn(\"publish\", df[\"publish\"].cast(\"date\")) \\\n",
    "           .withColumn(\"rating\", df[\"rating\"].cast(\"float\")) \\\n",
    "           .withColumn(\"ratingcount\", df[\"ratingcount\"].cast(\"int\")) \\\n",
    "           .withColumn(\"reviews\", df[\"reviews\"].cast(\"int\"))\n",
    "    \n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = connect_kafka()\n",
    "df = load_data(spark)\n",
    "df = format_data(df)\n",
    "df = convert(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.readStream.format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "        .option(\"subscribe\", \"goodread\") \\\n",
    "        .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DateType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `schema` cannot be resolved. Did you mean one of the following? [`value`].; line 1 pos 17;\n'Project ['from_json(value#533, 'schema) AS data#535]\n+- Project [cast(value#520 as string) AS value#533]\n   +- Relation [key#519,value#520,topic#521,partition#522,offset#523L,timestamp#524,timestampType#525] KafkaRelation(strategy=Subscribe[goodread], start=EarliestOffsetRangeLimit, end=LatestOffsetRangeLimit)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 74\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# Gọi hàm chính\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 74\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspark\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[17], line 63\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(spark)\u001b[0m\n\u001b[0;32m     60\u001b[0m kafka_df \u001b[38;5;241m=\u001b[39m read_from_kafka(spark)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# Chuyển đổi DataFrame từ JSON string sang DataFrame với schema đã định nghĩa\u001b[39;00m\n\u001b[1;32m---> 63\u001b[0m book_df \u001b[38;5;241m=\u001b[39m \u001b[43mkafka_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselectExpr\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrom_json(value, schema) as data\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \\\n\u001b[0;32m     64\u001b[0m                   \u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata.*\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# Chuyển đổi kiểu dữ liệu\u001b[39;00m\n\u001b[0;32m     67\u001b[0m converted_df \u001b[38;5;241m=\u001b[39m convert(book_df)\n",
      "File \u001b[1;32mc:\\Users\\kietn\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\dataframe.py:3267\u001b[0m, in \u001b[0;36mDataFrame.selectExpr\u001b[1;34m(self, *expr)\u001b[0m\n\u001b[0;32m   3265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(expr) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(expr[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m   3266\u001b[0m     expr \u001b[38;5;241m=\u001b[39m expr[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[1;32m-> 3267\u001b[0m jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselectExpr\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jseq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3268\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[1;32mc:\\Users\\kietn\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\kietn\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `schema` cannot be resolved. Did you mean one of the following? [`value`].; line 1 pos 17;\n'Project ['from_json(value#533, 'schema) AS data#535]\n+- Project [cast(value#520 as string) AS value#533]\n   +- Relation [key#519,value#520,topic#521,partition#522,offset#523L,timestamp#524,timestampType#525] KafkaRelation(strategy=Subscribe[goodread], start=EarliestOffsetRangeLimit, end=LatestOffsetRangeLimit)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, IntegerType, DateType\n",
    "\n",
    "# Khởi tạo Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Kafka Book Data Processing\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Định nghĩa schema cho DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"author\", StringType(), True),\n",
    "    StructField(\"bookUrl\", StringType(), True),\n",
    "    StructField(\"bookname\", StringType(), True),\n",
    "    StructField(\"describe\", StringType(), True),\n",
    "    StructField(\"prices\", StringType(), True),\n",
    "    StructField(\"publish\", StringType(), True),\n",
    "    StructField(\"rating\", StringType(), True),\n",
    "    StructField(\"ratingcount\", StringType(), True),\n",
    "    StructField(\"reviews\", StringType(), True),\n",
    "    StructField(\"fivestars\", StringType(), True),\n",
    "    StructField(\"fourstars\", StringType(), True),\n",
    "    StructField(\"threestars\", StringType(), True),\n",
    "    StructField(\"twostars\", StringType(), True),\n",
    "    StructField(\"onestar\", StringType(), True),\n",
    "    StructField(\"pages\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Giả sử bạn đọc dữ liệu từ Kafka vào một DataFrame\n",
    "def read_from_kafka(spark):\n",
    "    # Cấu hình đọc từ Kafka (cập nhật với cấu hình của bạn)\n",
    "    kafka_df = spark.read \\\n",
    "        .format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "        .option(\"subscribe\", \"goodread\") \\\n",
    "        .load()\n",
    "\n",
    "    # Chọn cột giá trị từ Kafka và chuyển đổi nó thành chuỗi\n",
    "    kafka_df = kafka_df.selectExpr(\"CAST(value AS STRING)\")\n",
    "    return kafka_df\n",
    "\n",
    "# Hàm chuyển đổi kiểu dữ liệu\n",
    "def convert(df):\n",
    "    df = df.withColumn(\"pages_n\", df[\"pages\"].cast(\"int\")) \\\n",
    "           .withColumn(\"prices\", df[\"prices\"].cast(\"float\")) \\\n",
    "           .withColumn(\"onestar\", df[\"onestar\"].cast(\"int\")) \\\n",
    "           .withColumn(\"twostars\", df[\"twostars\"].cast(\"int\")) \\\n",
    "           .withColumn(\"threestars\", df[\"threestars\"].cast(\"int\")) \\\n",
    "           .withColumn(\"fourstars\", df[\"fourstars\"].cast(\"int\")) \\\n",
    "           .withColumn(\"fivestars\", df[\"fivestars\"].cast(\"int\")) \\\n",
    "           .withColumn(\"publish\", df[\"publish\"].cast(\"date\")) \\\n",
    "           .withColumn(\"rating\", df[\"rating\"].cast(\"float\")) \\\n",
    "           .withColumn(\"ratingcount\", df[\"ratingcount\"].cast(\"int\")) \\\n",
    "           .withColumn(\"reviews\", df[\"reviews\"].cast(\"int\"))\n",
    "\n",
    "    return df\n",
    "\n",
    "# Hàm chính\n",
    "def main(spark):\n",
    "    # Đọc dữ liệu từ Kafka\n",
    "    kafka_df = read_from_kafka(spark)\n",
    "\n",
    "    # Chuyển đổi DataFrame từ JSON string sang DataFrame với schema đã định nghĩa\n",
    "    book_df = kafka_df.selectExpr(\"from_json(value, schema) as data\") \\\n",
    "                      .select(\"data.*\")\n",
    "\n",
    "    # Chuyển đổi kiểu dữ liệu\n",
    "    converted_df = convert(book_df)\n",
    "\n",
    "    # Hiển thị DataFrame sau khi chuyển đổi\n",
    "    converted_df.show()\n",
    "\n",
    "# Gọi hàm chính\n",
    "if __name__ == \"__main__\":\n",
    "    main(spark)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

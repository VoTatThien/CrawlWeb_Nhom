{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "def create_database():\n",
    "    # Connect to the default database to create the new database\n",
    "    conn = psycopg2.connect(\n",
    "        dbname='postgres',  # Connect to the default database\n",
    "        user='admin',\n",
    "        password='admin',\n",
    "        host='localhost',\n",
    "        port='5432'\n",
    "    )\n",
    "    conn.autocommit = True  # Enable autocommit to create database\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    try:\n",
    "        # Create the goodread database if it doesn't exist\n",
    "        cur.execute(\"SELECT 1 FROM pg_catalog.pg_database WHERE datname = 'goodread'\")\n",
    "        exists = cur.fetchone()\n",
    "        if not exists:\n",
    "            cur.execute(\"CREATE DATABASE goodread;\")\n",
    "            print(\"Database 'goodread' has been created successfully.\")\n",
    "        else:\n",
    "            print(\"Database 'goodread' already exists.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while creating database: {e}\")\n",
    "    finally:\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "\n",
    "def create_tables():\n",
    "    conn = psycopg2.connect(\n",
    "        dbname='goodread',\n",
    "        user='admin',\n",
    "        password='admin',\n",
    "        host='localhost',\n",
    "        port='5432'\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        \n",
    "        # Create Authors table\n",
    "        create_authors_table = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS authors (\n",
    "            author_id SERIAL PRIMARY KEY,\n",
    "            author_name VARCHAR(255) UNIQUE\n",
    "        );\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create Ratings table\n",
    "        create_ratings_table = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS ratings (\n",
    "            rating_id SERIAL PRIMARY KEY,\n",
    "            rating FLOAT,\n",
    "            fivestars INT,\n",
    "            fourstars INT,\n",
    "            threestars INT,\n",
    "            twostars INT,\n",
    "            onestar INT\n",
    "        );\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create Books table\n",
    "        create_books_table = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS books (\n",
    "            book_id SERIAL PRIMARY KEY,\n",
    "            rating_id INT,\n",
    "            author_id INT,\n",
    "            bookname VARCHAR(255),\n",
    "            publish DATE,\n",
    "            prices FLOAT,\n",
    "            rating FLOAT,\n",
    "            rating_count INT,\n",
    "            reviews INT,\n",
    "            pages_n INT,\n",
    "            cover VARCHAR(50),\n",
    "            bookUrl VARCHAR(255),\n",
    "            FOREIGN KEY (author_id) REFERENCES authors(author_id),\n",
    "            FOREIGN KEY (rating_id) REFERENCES ratings(rating_id)\n",
    "        );\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create Details table\n",
    "        create_details_table = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS details (\n",
    "            book_id INT PRIMARY KEY,\n",
    "            author_id INT,\n",
    "            describe TEXT,\n",
    "            book_title VARCHAR(255),\n",
    "            FOREIGN KEY (book_id) REFERENCES books(book_id),\n",
    "            FOREIGN KEY (author_id) REFERENCES authors(author_id)\n",
    "        );\n",
    "        \"\"\"\n",
    "        \n",
    "        # Execute table creation commands\n",
    "        cur.execute(create_authors_table)\n",
    "        cur.execute(create_ratings_table)\n",
    "        cur.execute(create_books_table)\n",
    "        cur.execute(create_details_table)\n",
    "        conn.commit()\n",
    "\n",
    "        print(\"Tables have been created successfully.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while creating tables: {e}\")\n",
    "    finally:\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "\n",
    "# Create the database and tables\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database 'goodread' has been created successfully.\n",
      "Tables have been created successfully.\n"
     ]
    }
   ],
   "source": [
    "create_database()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables have been created successfully.\n"
     ]
    }
   ],
   "source": [
    "create_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "def create_connection():\n",
    "    return psycopg2.connect(\n",
    "        dbname='goodread',\n",
    "        user='admin',\n",
    "        password='admin',\n",
    "        host='localhost',\n",
    "        port='5432'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_authors():\n",
    "    authors_data = [\n",
    "        ('J.K. Rowling',),\n",
    "        ('George Orwell',),\n",
    "        ('J.R.R. Tolkien',)\n",
    "    ]\n",
    "    \n",
    "    conn = create_connection()\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    try:\n",
    "        for author in authors_data:\n",
    "            cur.execute(\"INSERT INTO authors (author_name) VALUES (%s) ON CONFLICT (author_name) DO NOTHING;\", author)\n",
    "        \n",
    "        conn.commit()\n",
    "        print(\"Authors data inserted successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error inserting authors: {e}\")\n",
    "        conn.rollback()\n",
    "    finally:\n",
    "        cur.close()\n",
    "        conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import regexp_replace, col, regexp_extract, when, date_format, to_date\n",
    "import psycopg2\n",
    "\n",
    "# Connect to Kafka\n",
    "def connect_kafka():\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName('SparkKafkaToPostgres') \\\n",
    "        .config('spark.jars.packages', \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.3,\"\n",
    "                                        \"org.postgresql:postgresql:42.5.0\") \\\n",
    "        .getOrCreate()\n",
    "    return spark\n",
    "\n",
    "# Load data from Kafka\n",
    "def load_data(spark):\n",
    "    df = spark.readStream.format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "        .option(\"subscribe\", \"goodread\") \\\n",
    "        .load()\n",
    "    return df\n",
    "\n",
    "# Format data\n",
    "def format_data(df):\n",
    "    df = df.selectExpr(\"CAST(value AS STRING)\") \\\n",
    "        .selectExpr(\"from_json(value, 'author STRING, bookUrl STRING, bookname STRING, describe STRING, prices STRING, publish STRING, rating STRING, ratingcount STRING, reviews STRING, fivestars STRING, fourstars STRING, threestars STRING, twostars STRING, onestar STRING, pages STRING') as jsonData\") \\\n",
    "        .select(\"jsonData.*\")\n",
    "\n",
    "    # Processing string data\n",
    "    df = df.withColumn(\"onestar\", regexp_replace(col(\"onestar\"), r\"[^\\d]\", \"\"))\n",
    "    df = df.withColumn(\"twostars\", regexp_replace(col(\"twostars\"), r\"[^\\d]\", \"\"))\n",
    "    df = df.withColumn(\"threestars\", regexp_replace(col(\"threestars\"), r\"[^\\d]\", \"\"))\n",
    "    df = df.withColumn(\"fourstars\", regexp_replace(col(\"fourstars\"), r\"[^\\d]\", \"\"))\n",
    "    df = df.withColumn(\"fivestars\", regexp_replace(col(\"fivestars\"), r\"[^\\d]\", \"\"))\n",
    "    df = df.withColumn(\"pages_n\", regexp_extract(col(\"pages\"), r\"(\\d+)\", 1))\n",
    "    df = df.withColumn(\"cover\", regexp_extract(col(\"pages\"), r\",\\s*(.*)\", 1))\n",
    "    df = df.withColumn(\"prices\", when(df[\"prices\"].like(\"Kindle%\"), regexp_extract(col(\"prices\"), r\"\\$(\\d+\\.\\d{2})\", 1)).otherwise(0))\n",
    "    df = df.withColumn(\"publish\", regexp_extract(col(\"publish\"), r\"(\\w+ \\d{1,2}, \\d{4})\", 1))\n",
    "    df = df.withColumn(\"publish\", to_date(col(\"publish\"), \"MMMM d, yyyy\"))\n",
    "    df = df.withColumn(\"publish\", date_format(col(\"publish\"), \"dd/MM/yyyy\"))\n",
    "    df = df.withColumn(\"ratingcount\", regexp_replace(col(\"ratingcount\"), \",\", \"\"))\n",
    "    df = df.withColumn(\"reviews\", regexp_replace(col(\"reviews\"), \",\", \"\"))\n",
    "    df = df.drop(\"pages\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# Convert data types\n",
    "def convert(df):\n",
    "    df = df.withColumn(\"pages_n\", df[\"pages_n\"].cast(\"int\")) \\\n",
    "           .withColumn(\"prices\", df[\"prices\"].cast(\"float\")) \\\n",
    "           .withColumn(\"onestar\", df[\"onestar\"].cast(\"int\")) \\\n",
    "           .withColumn(\"twostars\", df[\"twostars\"].cast(\"int\")) \\\n",
    "           .withColumn(\"threestars\", df[\"threestars\"].cast(\"int\")) \\\n",
    "           .withColumn(\"fourstars\", df[\"fourstars\"].cast(\"int\")) \\\n",
    "           .withColumn(\"fivestars\", df[\"fivestars\"].cast(\"int\")) \\\n",
    "           .withColumn(\"publish\", df[\"publish\"].cast(\"date\")) \\\n",
    "           .withColumn(\"rating\", df[\"rating\"].cast(\"float\")) \\\n",
    "           .withColumn(\"ratingcount\", df[\"ratingcount\"].cast(\"int\")) \\\n",
    "           .withColumn(\"reviews\", df[\"reviews\"].cast(\"int\"))\n",
    "    \n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = connect_kafka()\n",
    "df = load_data(spark)\n",
    "df = format_data(df)\n",
    "df = convert(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.readStream.format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "        .option(\"subscribe\", \"goodread\") \\\n",
    "        .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DateType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `schema` cannot be resolved. Did you mean one of the following? [`value`].; line 1 pos 17;\n'Project ['from_json(value#533, 'schema) AS data#535]\n+- Project [cast(value#520 as string) AS value#533]\n   +- Relation [key#519,value#520,topic#521,partition#522,offset#523L,timestamp#524,timestampType#525] KafkaRelation(strategy=Subscribe[goodread], start=EarliestOffsetRangeLimit, end=LatestOffsetRangeLimit)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 74\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# Gọi hàm chính\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 74\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspark\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[17], line 63\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(spark)\u001b[0m\n\u001b[0;32m     60\u001b[0m kafka_df \u001b[38;5;241m=\u001b[39m read_from_kafka(spark)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# Chuyển đổi DataFrame từ JSON string sang DataFrame với schema đã định nghĩa\u001b[39;00m\n\u001b[1;32m---> 63\u001b[0m book_df \u001b[38;5;241m=\u001b[39m \u001b[43mkafka_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselectExpr\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrom_json(value, schema) as data\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \\\n\u001b[0;32m     64\u001b[0m                   \u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata.*\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# Chuyển đổi kiểu dữ liệu\u001b[39;00m\n\u001b[0;32m     67\u001b[0m converted_df \u001b[38;5;241m=\u001b[39m convert(book_df)\n",
      "File \u001b[1;32mc:\\Users\\kietn\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\dataframe.py:3267\u001b[0m, in \u001b[0;36mDataFrame.selectExpr\u001b[1;34m(self, *expr)\u001b[0m\n\u001b[0;32m   3265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(expr) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(expr[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m   3266\u001b[0m     expr \u001b[38;5;241m=\u001b[39m expr[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[1;32m-> 3267\u001b[0m jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselectExpr\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jseq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3268\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[1;32mc:\\Users\\kietn\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\kietn\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `schema` cannot be resolved. Did you mean one of the following? [`value`].; line 1 pos 17;\n'Project ['from_json(value#533, 'schema) AS data#535]\n+- Project [cast(value#520 as string) AS value#533]\n   +- Relation [key#519,value#520,topic#521,partition#522,offset#523L,timestamp#524,timestampType#525] KafkaRelation(strategy=Subscribe[goodread], start=EarliestOffsetRangeLimit, end=LatestOffsetRangeLimit)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, IntegerType, DateType\n",
    "\n",
    "# Khởi tạo Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Kafka Book Data Processing\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Định nghĩa schema cho DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"author\", StringType(), True),\n",
    "    StructField(\"bookUrl\", StringType(), True),\n",
    "    StructField(\"bookname\", StringType(), True),\n",
    "    StructField(\"describe\", StringType(), True),\n",
    "    StructField(\"prices\", StringType(), True),\n",
    "    StructField(\"publish\", StringType(), True),\n",
    "    StructField(\"rating\", StringType(), True),\n",
    "    StructField(\"ratingcount\", StringType(), True),\n",
    "    StructField(\"reviews\", StringType(), True),\n",
    "    StructField(\"fivestars\", StringType(), True),\n",
    "    StructField(\"fourstars\", StringType(), True),\n",
    "    StructField(\"threestars\", StringType(), True),\n",
    "    StructField(\"twostars\", StringType(), True),\n",
    "    StructField(\"onestar\", StringType(), True),\n",
    "    StructField(\"pages\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Giả sử bạn đọc dữ liệu từ Kafka vào một DataFrame\n",
    "def read_from_kafka(spark):\n",
    "    # Cấu hình đọc từ Kafka (cập nhật với cấu hình của bạn)\n",
    "    kafka_df = spark.read \\\n",
    "        .format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "        .option(\"subscribe\", \"goodread\") \\\n",
    "        .load()\n",
    "\n",
    "    # Chọn cột giá trị từ Kafka và chuyển đổi nó thành chuỗi\n",
    "    kafka_df = kafka_df.selectExpr(\"CAST(value AS STRING)\")\n",
    "    return kafka_df\n",
    "\n",
    "# Hàm chuyển đổi kiểu dữ liệu\n",
    "def convert(df):\n",
    "    df = df.withColumn(\"pages_n\", df[\"pages\"].cast(\"int\")) \\\n",
    "           .withColumn(\"prices\", df[\"prices\"].cast(\"float\")) \\\n",
    "           .withColumn(\"onestar\", df[\"onestar\"].cast(\"int\")) \\\n",
    "           .withColumn(\"twostars\", df[\"twostars\"].cast(\"int\")) \\\n",
    "           .withColumn(\"threestars\", df[\"threestars\"].cast(\"int\")) \\\n",
    "           .withColumn(\"fourstars\", df[\"fourstars\"].cast(\"int\")) \\\n",
    "           .withColumn(\"fivestars\", df[\"fivestars\"].cast(\"int\")) \\\n",
    "           .withColumn(\"publish\", df[\"publish\"].cast(\"date\")) \\\n",
    "           .withColumn(\"rating\", df[\"rating\"].cast(\"float\")) \\\n",
    "           .withColumn(\"ratingcount\", df[\"ratingcount\"].cast(\"int\")) \\\n",
    "           .withColumn(\"reviews\", df[\"reviews\"].cast(\"int\"))\n",
    "\n",
    "    return df\n",
    "\n",
    "# Hàm chính\n",
    "def main(spark):\n",
    "    # Đọc dữ liệu từ Kafka\n",
    "    kafka_df = read_from_kafka(spark)\n",
    "\n",
    "    # Chuyển đổi DataFrame từ JSON string sang DataFrame với schema đã định nghĩa\n",
    "    book_df = kafka_df.selectExpr(\"from_json(value, schema) as data\") \\\n",
    "                      .select(\"data.*\")\n",
    "\n",
    "    # Chuyển đổi kiểu dữ liệu\n",
    "    converted_df = convert(book_df)\n",
    "\n",
    "    # Hiển thị DataFrame sau khi chuyển đổi\n",
    "    converted_df.show()\n",
    "\n",
    "# Gọi hàm chính\n",
    "if __name__ == \"__main__\":\n",
    "    main(spark)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 11\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkafka\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KafkaConsumer\n\u001b[0;32m      3\u001b[0m consumer \u001b[38;5;241m=\u001b[39m KafkaConsumer(\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgoodread\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      5\u001b[0m     bootstrap_servers\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocalhost:9092\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      8\u001b[0m     group_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmy-group\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      9\u001b[0m )\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m consumer:\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKey: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage\u001b[38;5;241m.\u001b[39mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Value: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\kietn\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\kafka\\consumer\\group.py:1193\u001b[0m, in \u001b[0;36mKafkaConsumer.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1191\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext_v1()\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1193\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext_v2\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kietn\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\kafka\\consumer\\group.py:1201\u001b[0m, in \u001b[0;36mKafkaConsumer.next_v2\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1199\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_message_generator_v2()\n\u001b[0;32m   1200\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1202\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m   1203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kietn\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\kafka\\consumer\\group.py:1116\u001b[0m, in \u001b[0;36mKafkaConsumer._message_generator_v2\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_message_generator_v2\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1115\u001b[0m     timeout_ms \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;241m*\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consumer_timeout \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mtime())\n\u001b[1;32m-> 1116\u001b[0m     record_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout_ms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdate_offsets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   1117\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m tp, records \u001b[38;5;129;01min\u001b[39;00m six\u001b[38;5;241m.\u001b[39miteritems(record_map):\n\u001b[0;32m   1118\u001b[0m         \u001b[38;5;66;03m# Generators are stateful, and it is possible that the tp / records\u001b[39;00m\n\u001b[0;32m   1119\u001b[0m         \u001b[38;5;66;03m# here may become stale during iteration -- i.e., we seek to a\u001b[39;00m\n\u001b[0;32m   1120\u001b[0m         \u001b[38;5;66;03m# different offset, pause consumption, or lose assignment.\u001b[39;00m\n\u001b[0;32m   1121\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m record \u001b[38;5;129;01min\u001b[39;00m records:\n\u001b[0;32m   1122\u001b[0m             \u001b[38;5;66;03m# is_fetchable(tp) should handle assignment changes and offset\u001b[39;00m\n\u001b[0;32m   1123\u001b[0m             \u001b[38;5;66;03m# resets; for all other changes (e.g., seeks) we'll rely on the\u001b[39;00m\n\u001b[0;32m   1124\u001b[0m             \u001b[38;5;66;03m# outer function destroying the existing iterator/generator\u001b[39;00m\n\u001b[0;32m   1125\u001b[0m             \u001b[38;5;66;03m# via self._iterator = None\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kietn\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\kafka\\consumer\\group.py:655\u001b[0m, in \u001b[0;36mKafkaConsumer.poll\u001b[1;34m(self, timeout_ms, max_records, update_offsets)\u001b[0m\n\u001b[0;32m    653\u001b[0m remaining \u001b[38;5;241m=\u001b[39m timeout_ms\n\u001b[0;32m    654\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 655\u001b[0m     records \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_records\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdate_offsets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mupdate_offsets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    656\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m records:\n\u001b[0;32m    657\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m records\n",
      "File \u001b[1;32mc:\\Users\\kietn\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\kafka\\consumer\\group.py:675\u001b[0m, in \u001b[0;36mKafkaConsumer._poll_once\u001b[1;34m(self, timeout_ms, max_records, update_offsets)\u001b[0m\n\u001b[0;32m    665\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_poll_once\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout_ms, max_records, update_offsets\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    666\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Do one round of polling. In addition to checking for new data, this does\u001b[39;00m\n\u001b[0;32m    667\u001b[0m \u001b[38;5;124;03m    any needed heart-beating, auto-commits, and offset updates.\u001b[39;00m\n\u001b[0;32m    668\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;124;03m        dict: Map of topic to list of records (may be empty).\u001b[39;00m\n\u001b[0;32m    674\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_coordinator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    677\u001b[0m     \u001b[38;5;66;03m# Fetch positions if we have partitions we're subscribed to that we\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[38;5;66;03m# don't know the offset for\u001b[39;00m\n\u001b[0;32m    679\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_subscription\u001b[38;5;241m.\u001b[39mhas_all_fetch_positions():\n",
      "File \u001b[1;32mc:\\Users\\kietn\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\kafka\\coordinator\\consumer.py:289\u001b[0m, in \u001b[0;36mConsumerCoordinator.poll\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    286\u001b[0m             metadata_update \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mcluster\u001b[38;5;241m.\u001b[39mrequest_update()\n\u001b[0;32m    287\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mpoll(future\u001b[38;5;241m=\u001b[39mmetadata_update)\n\u001b[1;32m--> 289\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mensure_active_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    291\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpoll_heartbeat()\n\u001b[0;32m    293\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_auto_commit_offsets_async()\n",
      "File \u001b[1;32mc:\\Users\\kietn\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\kafka\\coordinator\\base.py:407\u001b[0m, in \u001b[0;36mBaseCoordinator.ensure_active_group\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    404\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    405\u001b[0m     future \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjoin_future\n\u001b[1;32m--> 407\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfuture\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfuture\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    409\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m future\u001b[38;5;241m.\u001b[39msucceeded():\n\u001b[0;32m    410\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_join_complete(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generation\u001b[38;5;241m.\u001b[39mgeneration_id,\n\u001b[0;32m    411\u001b[0m                            \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generation\u001b[38;5;241m.\u001b[39mmember_id,\n\u001b[0;32m    412\u001b[0m                            \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generation\u001b[38;5;241m.\u001b[39mprotocol,\n\u001b[0;32m    413\u001b[0m                            future\u001b[38;5;241m.\u001b[39mvalue)\n",
      "File \u001b[1;32mc:\\Users\\kietn\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\kafka\\client_async.py:602\u001b[0m, in \u001b[0;36mKafkaClient.poll\u001b[1;34m(self, timeout_ms, future)\u001b[0m\n\u001b[0;32m    599\u001b[0m             timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(timeout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mretry_backoff_ms\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    600\u001b[0m         timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, timeout)  \u001b[38;5;66;03m# avoid negative timeouts\u001b[39;00m\n\u001b[1;32m--> 602\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    604\u001b[0m \u001b[38;5;66;03m# called without the lock to avoid deadlock potential\u001b[39;00m\n\u001b[0;32m    605\u001b[0m \u001b[38;5;66;03m# if handlers need to acquire locks\u001b[39;00m\n\u001b[0;32m    606\u001b[0m responses\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fire_pending_completed_requests())\n",
      "File \u001b[1;32mc:\\Users\\kietn\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\kafka\\client_async.py:634\u001b[0m, in \u001b[0;36mKafkaClient._poll\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_register_send_sockets()\n\u001b[0;32m    633\u001b[0m start_select \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 634\u001b[0m ready \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    635\u001b[0m end_select \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sensors:\n",
      "File \u001b[1;32mc:\\Users\\kietn\\AppData\\Local\\Programs\\Python\\Python310\\lib\\selectors.py:324\u001b[0m, in \u001b[0;36mSelectSelector.select\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    322\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 324\u001b[0m     r, w, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_select\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_readers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_writers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "File \u001b[1;32mc:\\Users\\kietn\\AppData\\Local\\Programs\\Python\\Python310\\lib\\selectors.py:315\u001b[0m, in \u001b[0;36mSelectSelector._select\u001b[1;34m(self, r, w, _, timeout)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_select\u001b[39m(\u001b[38;5;28mself\u001b[39m, r, w, _, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 315\u001b[0m     r, w, x \u001b[38;5;241m=\u001b[39m \u001b[43mselect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m r, w \u001b[38;5;241m+\u001b[39m x, []\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from kafka import KafkaConsumer\n",
    "\n",
    "consumer = KafkaConsumer(\n",
    "    'goodread',\n",
    "    bootstrap_servers=['localhost:9092'],\n",
    "    auto_offset_reset='earliest',\n",
    "    enable_auto_commit=True,\n",
    "    group_id='my-group'\n",
    ")\n",
    "\n",
    "for message in consumer:\n",
    "    print(f\"Key: {message.key}, Value: {message.value.decode('utf-8')}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
